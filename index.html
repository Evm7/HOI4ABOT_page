<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta property="og:title" content="HOI4ABOT: Human-Object Interaction Anticipation for Assistive roBOTs"/>
  <meta property="og:url" content="https://evm7.github.io/HOI4ABOT/"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <title>HOI4ABOT: Human-Object Interaction Anticipation for Assistive roBOTs</title>
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="icon" href="static/figures/icon2.png">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://evm7.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
            <a class="navbar-item" href="https://evm7.github.io/Self-AWare/">
            Self-AWare
          </a>
          <a class="navbar-item" href="https://evm7.github.io/I-CTRL/">
            I-CTRL
          </a>
          <a class="navbar-item" href="https://evm7.github.io/ECHO/">
            ECHO
          </a>
          <a class="navbar-item" href="https://evm7.github.io/UnsH2R/">
            ImitationNet
          </a>
          <a class="navbar-item" href="https://evm7.github.io/UNIMASKM-page/">
            UNIMASK-M
          </a>
          <a class="navbar-item" href="https://evm7.github.io/icvae-page/">
            IntentionCVAE
          </a>
          <a class="navbar-item" href="https://evm7.github.io/HOIGaze-page/">
            HOIGaze
          </a>
            <a class="navbar-item" href="https://evm7.github.io/2CHTR-page/">
            2CHTR
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="publication-header">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- <div class="columns is-centered"> -->
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HOI4ABOT: Human-Object Interaction Anticipation for Assistive roBOTs</h1>
          <div class="is-size-3 publication-authors">
            Conference on Robot Learning 2023
          </div>
        </div>
    </div>
  </div>

</section>

<section class="publication-author-block">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://evm7.github.io/" target="_blank">Esteve Valls Mascaro</a>,</span>
            <span class="author-block"><a href="https://www.tuwien.at/etit/ict/asl/team/daniel-sliwowski" target="_blank">Daniel Sliwowski</a>,</span>
            <span class="author-block"><a href="https://www.tuwien.at/etit/ict/asl/team/dongheui-lee" target="_blank">Dongheui Lee</a>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Technische Universitaet Wien (TUWien), German Aerospace Center (DLR)</span> 
            <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
          </div>
          

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://openreview.net/forum?id=rYZBdBytxBx" target="_blank"
                  class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>OpenReview</span>
                </a>
              </span>

              
              <!-- PDF Link. -->
<!--              <span class="link-block">-->
<!--                <a href="static/source/ADDHEREPDF.pdf" target="_blank"-->
<!--                  class="external-link button is-normal is-rounded">-->
<!--                  <span class="icon">-->
<!--                    <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
<!--                            </span>-->
              <!-- </span> -->
              <!-- Colab Link. -->
<!--              <span class="link-block">-->
<!--                <a href="ADD HERE THE CODE" target="_blank"-->
<!--                class="external-link button is-normal is-rounded">-->
<!--                <span class="icon">-->
<!--                  <i class="fab fa-github"></i>-->
<!--                </span>-->
<!--                <span>Code</span>-->
<!--              </a>-->
<!--             </span>-->

<!--              <span class="link-block">-->
<!--                <a href="ADD HERE REPLICATE IF NEEDED" target="_blank"-->
<!--                class="external-link button is-normal is-rounded">-->
<!--                <span class="icon">-->
<!--                  <i class="fas fa-rocket"></i>-->
<!--                </span>-->
<!--                <span>Demo</span>-->
<!--              </a>-->
<!--              </span>-->
              <!-- </span> -->
              <!-- Colab Link. -->
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container">
      <div class="item">
      <div class="column is-centered has-text-centered">
        <img src="static/figures/Overviewv2.png" alt="HOI4ABOT"/>
      </div>
    </div>
  </div>
  </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robots are becoming increasingly integrated into our lives, assisting us in various tasks. To ensure effective collaboration between humans and robots, it is essential that they understand our intentions and anticipate our actions. In this paper, we propose a Human-Object Interaction (HOI) anticipation framework for Assistive Robots. We propose an efficient and robust transformer-based model to detect and anticipate HOIs from videos. This enhanced anticipation empowers robots to proactively assist humans, resulting in more efficient and intuitive collaborations. Our model outperforms state-of-the-art results in HOI detection and anticipation in VidHOI dataset with an increase of 1.76% and 1.04% in mAP respectively while being 15.4 times faster. We showcase the effectiveness of our approach through experimental results in a real robot, demonstrating that the robot’s ability to anticipate HOIs is key for better Human-Robot Interaction
</p>
        </div>
                
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

  <section class="hero is-small">
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container">
      <div class="item">
          <p style="margin-bottom: 30px">
 
            <video poster="" id="tree" autoplay controls muted loop height="100%">
              <source src="static/figures/main.mp4"
              type="video/mp4">
            </video>
          </p>
        </div>
  </div>
  </div>
  </div>
</section>


  <section class="section hero is-light">
<div class="container is-max-desktop">
  <!-- Abstract. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">How does it work?</h2>
      <div class="content has-text-justified">
        <p>
Our two-stage model predicts HOI in videos. Here, we consider a video of T + 1 frames with the pre-extracted object and human bounding boxes. Our module initially extracts relevant features per frame (left) to later on detect and anticipate HOIs (right). First, we leverage a pre-trained ViT architecture [45] to extract patch-based local features Et and also global context features clst per each frame. Then, we obtain individual features per human et n and object et m by aligning Et to their bounding boxes through our Patch Merger strategy, shown in blue. We also project each bounding box Bt to ˆBt using a box embedder [46], and the object category to a semantic feature using CLIP [47]. We finalize the feature extraction by averaging all clst in time. Our Dual Transformer module, shown in purple, leverages the human and
object-constructed windows (sequences in red and blue respectively) through two cross-attention transformers. Finally, we project the enhanced last feature from the Human Blender to detect and anticipate HOIs at several time horizons iτ k in the future through our Hydra head (shown in green)
</p>
      </div>
              
    </div>
  </div>
  <!--/ Abstract. -->
</div>
</section>
    
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="column is-centered has-text-centered">
        <img src="static/figures/Architecture.jpg" alt="Architecture of our HOI4ABOT model"/>
      </div>    
  </div>
</div>
</div>
</section>


  <section class="section hero is-light">
<div class="container is-max-desktop">
  <!-- Abstract. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Qualitative Results</h2>
      <div class="content has-text-justified">
        <p>
HOI detection and anticipation are essential for robots to comprehend the surrounding humans and better predict their needs, so the robot can assist in a timely manner. We conduct real experiments with a Franka Emika Panda robot to showcase the benefit of our approach in assistive robots beyond the offline VidHOI dataset.  We consider the ‘pouring task’ in a kitchen scenario where the robot assumes the role of a bartender with the goal of pouring a beverage for the human. Next we showcase different experiments to demonstrate the robustness of our HOI4ABOT to detecting and anticipating HOIs for multiple people and multiple objects, in a cluttered scenario and also with more complex robot tasks.
</p>
      </div>
              
    </div>
  </div>
  <!--/ Abstract. -->
</div>
</section>
      <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">

        <div class="column is-centered has-text-centered">
          <p><b> “Complex scenario”</b></p>
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="static/figures/videos/Complex_scenario.mp4" type="video/mp4">
          </video>
        </div>
               

        <div class="column is-centered has-text-centered">
          <p><b> “Different robot actions: object handover”</b></p>
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="static/figures/videos/push.mp4" type="video/mp4">
          </video>
        </div>
        

        <div class="column is-centered has-text-centered">
          <p><b> “Distinguishing between multiple objects and people”</b></p>
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="static/figures/videos/Multiple1.mp4" type="video/mp4">
          </video>
        </div>
        
          </div>
        </div>
      </div>
    </section>

  
  

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@inproceedings{
mascaro2023hoiabot,
title={{HOI}4{ABOT}: Human-Object Interaction Anticipation for Human Intention Reading Assistive ro{BOT}s},
author={Esteve Valls Mascaro and Daniel Sliwowski and Dongheui Lee},
booktitle={7th Annual Conference on Robot Learning},
year={2023},
url={https://openreview.net/forum?id=rYZBdBytxBx}
}</code></pre>
    </div>
</section>




<footer class="footer">
 <!--  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
      href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
      <i class="fas fa-file-pdf"></i>
    </a>
    <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
      <i class="fab fa-github"></i>
    </a>
  </div> -->
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
        href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
      </p>
    </div>
  </div>
</div>
</div>
</footer>

  </body>
  </html>
